---
title: Using Gaussian Mixture Models to Deconstruct Data With Unusual Distributions

# to produce blinded version set to 1
blinded: 0

authors: 
- name: Cassandra Jin
  thanks: The author gratefully acknowledges Professor Amy Wagaman for her enriching guidance and patient support in STAT-495. Professor Wagaman's instruction throughout the course has allowed for an incredible amount of technical and personal growth, and this project is has been a wonderful opportunity to apply my skills as well as to learn more invaluable ones.
  affiliation: Department of Mathematics and Statistics, Amherst College
  

keywords:
- Probability density function; Expectation maximization algorithm; Multimodality; Pattern recognition; Covariance; k-means clustering

abstract: |
  This paper explores the fundamental concepts that underpin Gaussian mixture models (GMM), offering a comprehensive overview of their mathematical formulation, the Expectation-Maximization (EM) algorithm employed for parameter estimation, and the principles that govern their probabilistic nature. The GMM is a generalization of k-means clustering and is used for decomposing multimodal data into multiple component distributions, namely Gaussian ones. This paper will thus include an overview of the Gaussian distribution and go deeper into the utility of covariance matrices. It will demonstrate how to implement GMM functions in R and how these apply to real-world applications where GMMs have demonstrated remarkable efficacy, illustrating their relevance and versatility across diverse domains.


bibliography: bibliography.bib
biblio-style: plainnat
output:
  rticles::asa_article:
---

```{r setup, include = FALSE}
library(tidyverse)
library(mosaic)
library(mdsr)
library(knitr)
library(kableExtra)
library(MGMM) # for fitting GMMs in the presence of missing data
library(mclust)
library(MASS) # for galaxies data
```

# Introduction

In the landscape of machine learning, pattern recognition, and data analysis, the objective and the challenge often lie in capturing intricate data distributions as effectively as possible. It may be convenient to assign a single distribution that seems to fit well enough to a large data set, but it is much more insightful to identify patterns and clusters within the data. Clustering algorithms such as k-means clustering provide solutions to finding the patterns within observations, but such traditional methods still have limitations when it comes to identifying clusters of different shapes and sizes. For working with a diverse data set, the Gaussian mixture model (GMM) employs a combination of multiple Gaussian distributions and, as a result, excels at capturing the various latent structures within the data [@kumar2022gaussian].

![A multimodal distribution, where each peak represents a different Gaussian distribution or the cluster in the dataset (Ravihara, 2023).](3.jpg)

We understand k-means clustering to be a method that partitions observations into $k$ clusters in which each observation belongs to the cluster with the nearest mean. Like k-means clustering, the GMM is a type of machine learning algorithm, but instead, it groups data into different categories based on any underlying probability distributions. By identifying and merging multiple Gaussian components, the GMM achieves a parametric probability density function represented as a weighted sum of the densities corresponding to each Gaussian component [@reynolds2009gaussian]. The parameters in the method are estimated from training data using the iterative expectation-maximization (EM) algorithm, and in maximizing likelihood, GMM allows a representation of data that goes beyond the limitations of single-distribution models [@bouguila2020mixture]. The method's ability to model a wide array of distributions (multimodal, non-Gaussian, etc.) has made GMMs essential in various domains such as computer vision, speech recognition, anomaly detection, finance, marketing, and more.

This paper aims to guide readers through the fundamental concepts that underpin GMMs, offering a comprehensive overview of their mathematical formulation, the Expectation-Maximization (EM) algorithm employed for parameter estimation, and the principles that govern their probabilistic nature. Furthermore, we will explore real-world applications where GMMs have demonstrated remarkable efficacy, illustrating their relevance and versatility across diverse domains.


# Gaussian Mixture Models (GMMs)
\label{sec:background}

## The Gaussian Distribution

The most basic concept in probability theory and in statistics is the random variable. A random variable can be understood as a mapping from a random experiment to a variable, and depending on the nature of the experiment and the design of the mapping, a random variable can take on discrete values, continuous values, or a mix of discrete and continuous values. One possible distribution that the resulting values may follow is the singular Gaussian distribution, which has probability density function (PDF)

\begin{align} R_X(t, s) &= E[A \cos \omega t \cdot A \cos \omega s] \\ &= A^2 E[\cos \omega t \cos \omega s] \end{align}

\begin{equation}
\label{single}
p(x) = \frac{1}{(2\pi)^{1/2} \sigma} exp(-\frac{1}{2}(\frac{x-\mu}{\sigma})^2) \dot{=} N(x; \mu, \sigma^2),
\end{equation}

or $x \sim N(\mu, \sigma^2)$, where $x$ is a continuous random variable obeying a normal distribution with mean $\mu$ and variance $\sigma^2$.

The Gaussian distribution is commonly used in many engineering and science disciplines due to its highly desirable computational properties, but also from its ability to approximate many naturally occurring real-world distributions, due to the law of large numbers (Ravihara, 2023).

Expanding this definition to incorporate the weights of component distributions, $c_m$, we have that a continuous random variable has a Gaussian-mixture distribution if its PDF is specified by

\begin{equation}
\label{mix}
p(x) = \sum_{m=1}^M \frac{c_m}{(2\pi)^{1/2} \sigma_m} exp(-\frac{1}{2}(\frac{x-\mu_m}{\sigma_m})^2) = \sum_{m=1}^M c_m N(\mu_m, \sigma_m^2),
\end{equation}

where $\infty < x < \infty$, $\sigma_m > 0$, and $c_m > 0$ [@yu2015gaussian].

These positive mixture weights must sum to 1: $\sum_{m=1}^M c_m = 1$, i.e. the component distributions combine to describe the full mixture model.

The most obvious property of Gaussian mixture distribution is its multimodality ($M>1$ in Equation \ref{mix}), in contrast to the unimodal property of the Gaussian distribution where $M=1$. This makes it possible for a GMM to adequately describe many types of physical data exhibiting multimodality, which are poorly suited for a single Gaussian distribution.

Based on Equation \ref{mix}, the expectation of a random variable $x$ with the mixture Gaussian PDF is $E(x) = \sum_{m=1}^M c_m \mu_m$. But unlike a singular, unimodal Gaussian distribution, this simple summary statistic is not very informative unless all the component means, $\mu_m$ for $m = 1, ..., M$, in the Gaussian-mixture distribution are close to each other.

Instead, we use the multivariate generalization of the mixture Gaussian distribution, which has joint PDF

\begin{equation}
\label{joint}
p(x) = \sum_{m=1}^M \frac{c_m}{(2\pi)^{D/2} |\Sigma_m|^{1/2}} exp(-\frac{1}{2}(x-\mu_m)^T \Sigma_m^{-1}(x-\mu_m)) = \sum_{m=1}^M c_m N(x; \mu_m, \Sigma_m),
\end{equation}

where $D$ is random variable $x$'s dimensionality, $\Sigma_m$ holds the covariance matrices, and $c_m > 0$ still.

In using the multivariate mixture Gaussian distribution of Equation \ref{mix}, if $D$ is large (this depends on the context), then the use of full (nondiagonal) covariance matrices ($\Sigma_m$) would involve a large number of parameters [@bouguila2020mixture]. To reduce the number of parameters, one can instead use diagonal covariance matrices for $\Sigma_m$. Alternatively, when $M$ is large, one can also constrain all covariance matrices to be the same. The advantage of using diagonal covariance matrices is significant simplification of computations needed for the applications of the Gaussian-mixture distributions [@wan2019novel].

## Parameter Estimation

The Gaussian-mixture distributions discussed above contain a set of parameters. In the multivariate case of Equation \ref{mix}, the parameter set consists of $\Theta = \{c_m, \mu_m, \Sigma_m\}$. The parameter estimation problem is motivated by wanting to determine the values of these parameters from data set that we assume to be drawn from the Gaussian-mixture distribution.

It is common to think of Gaussian mixture modeling and the related parameter estimation as a missing data problem. We want to "learn" appropriate parameters for the distribution, with the connection to the data points being represented as their membership in the individual Gaussian distributions.

Here, we focus on the expectation maximization (EM) algorithm as the maximum likelihood method of choice for parameter estimation of the Gaussian-mixture distribution. The EM algorithm is the most popular technique used to estimate the parameters of a mixture given a fixed number of mixture components, and it can be used to compute the parameters of any parametric mixture distribution. The EM algorithm finds the maximum likelihood of a model through two iterative steps: an expectation or E-step and a maximization or M-step. It alternates between performing an expectation step and a maximization step to achieve maximum likelihood until a certain stop condition is satisfied or a specified number of iterations is completed [@wan2019novel].

The EM algorithm is especially fitting to use for GMM, as we can express the parameters in their closed forms and iterate through the M-step [@do2008expectation]. Given the current estimate for the parameters, denoted by just a superscript in the closed forms, the conditional probability for a given observation $x^{(t)}$ generated from a mixture component $m$ is determined for each data sample point at $t=1, ..., N$, where $N$ is the sample size. The parameters then update such that the new component weights correspond to the average conditional probability, and each component mean and covariance is the component specific weighted average of the mean and covariance of the entire sample set.

A particular advantage of the EM algorithm is that each successive iteration will not decrease the likelihood, a property not shared by many other maximization techniques [@reynolds2009gaussian]. Additionally, the EM algorithm naturally embeds constraints on the probability vector while avoiding extra computational costs to check and maintain appropriate values [@yu2015gaussian]. On the other hand, the EM algorithm can be too sensitive to initial values and may falsely identify local maxima. This problem can be addressed by evaluating at several initial points in the parameter space, but this may become computationally costly and undo the advantageous efficiency of the algorithm.

## Summary

Overall, there are three key steps to using Gaussian mixture models for clustering:

1. Determine a covariance matrix that defines how each Gaussian is related to one another. The more similar two Gaussian component distributions are, the closer their means will be and vice versa if they are far away from each other in terms of similarity. A GMM can have a covariance matrix that is diagonal or symmetric (non-diagonal).

2. Decide the number of Gaussian component distributions, which dictates how many clusters are present in the model.

3. Select the hyperparameters that define how the data are optimally separated using GMMs.


# Connections and Comparisons to Other Models
\label{sec:compare}

Compared with traditional k-means clustering algorithm, GMM has two primary advantages. Firstly, it takes into account covariance, which determines the shape of the distribution, so it can fit well when the distribution of the data presents different shapes. Secondly, while k-means results in hard clustering, GMM is able to perform a more flexible clustering on data, called soft clustering ("Cluster Using Gaussian Mixture Model", 2023). Soft clustering methods assign a score to each data point in a cluster to represent the association strength of the data point to the cluster. As opposed to how hard clustering (k-means) outputs a specific category for each data point, soft clustering (GMM) is flexible in that it can assign a data point to more than one cluster and it outputs the probability of each specific category the data points belong to. Thus, the GMM yields an estimated uncertainty measure of the degree of association between data points and specific categories [@huang2023gaussian].

As a trade-off of GMMs being more flexible than k-means, they can be more difficult to train. K-means is typically faster to converge and more accurate, so it may be preferred in cases where the runtime is an important consideration or generally when the data set is large and the clusters are well-separated. On the other hand, GMMs are more accurate when the data set is small or the clusters are not well-separated [@kumar2022gaussian].

We know that the EM algorithm enables effective parameter estimation in probabilistic models with incomplete data. By implementing this algorithm, Gaussian mixture models can handle missing data, whereas K-means cannot. This difference makes GMMs more effective in certain applications where the data contains considerable noise or is not well-defined.


# Applications in Literature
\label{sec:examples}

* Understanding the impact of COVID-19 on energy consumption: Researchers used the electricity data set of public buildings in Scotland and applied Gaussian Mixture Model (GMM) to explore the changes in electricity usage patterns throughout the pandemic, so as to understand the long-term impact of COVID-19 on energy consumption of public buildings [@huang2023gaussian].

![Daily electricity consumption trends of various public buildings in different periods [@huang2023gaussian].](1.jpg)

![On the right, (b), the visualized Gaussian mixture model for each condition [@huang2023gaussian].](2.jpg)

* Finding patterns in medical datasets: Medical researchers use GMMs to segment images into multiple categories based on their content or to find specific patterns in medical datasets. They can determine clusters of patients with similar symptoms, identify disease subtypes, and even predict outcomes [@riaz2020gaussian].

* Modeling natural phenomena: GMMs can be used to model natural phenomena where it has been found that noise follows Gaussian distributions. This model of probabilistic modeling relies on the assumption that there exists some underlying continuum of unobserved entities or attributes, and that each member is associated with measurements taken at equidistant points in multiple observation sessions [@xi2023deep].

* Customer behavior analysis: GMMs can be used for performing customer behavior analysis in marketing to make predictions about future purchases based on historical data [@melzi2017dedicated].

* Stock price prediction: In finance, GMMs can be applied to a stockâ€™s price time series. They can detect changepoints in time series data and help find turning points of stock prices or other market movements that are otherwise difficult to spot due to volatility and noise [@gopinathan2023stock].

* Gene expression data analysis: GMMs can be used to detect differentially expressed genes between two conditions and to identify which genes might contribute toward a certain phenotype or disease state [@mcnicholas2010model].


# Implementation in R

The function `rGMM` simulates observations from a GMM. The number of observations is specified by $n$, and the dimension of each observation by $d$, as seen in Equation \ref{joint}. The number of clusters is set using $k$, which defaults to 1. We introduce the parameter of missingness, which is the proportion of elements in the $n \times d$ data matrix that are missing and which defaults to zero [@McCaw2019.12.20.884551]. The cluster means and covariances (`covs`) are provided as numeric prototype vectors, or lists of such vectors. In settings where `miss` $> 0$ (it is possible for all elements of an observation to be missing), these vectors are used as the mean and covariance of all clusters. By default, however, the zero vector and the identity matrix are adopted for the mean and covariance, respectively. We will simulate a few cases of data to demonstrate the functionality of the `rGMM` function.

## Single Component Without Missingness

For $n$ observations on $d$ dimensional data, we can introduce a fraction of missing values $m$ completely at random by setting elements of the data set to NA.

Here, $n = 1000$ observations are simulated from a single $k = 1$ bivariate normal distribution $d = 2$ without missingness (miss = 0, by default). The mean is $\mu = (2, 2)$, and the covariance is an exchangeable correlation structure with off-diagonal $\rho = 0.5$.

```{r}
set.seed(495)
sigma <- matrix(c(1, 0.5, 0.5, 1), nrow = 2)
data = rGMM(n = 1e3, d = 2, k = 1, means = c(2, 2), covs = sigma)
fit <- FitGMM(data, k = 1)
show(fit)
```

The `Estimated covariance` table contains values obtained through fitting a GMM with the `rGMM` function on generated data, and we see that they are close to the target covariance values of {1, 0.5, 0.5, 1}.

## Single Component With Missingness

$n = 1000$ observations are simulated from a single $k = 1$ trivariate normal distribution $d = 3$ with 20% missingness (miss = 0.2). The mean defaults to the zero vector, and the covariance to the identity matrix.

```{r}
set.seed(495)
sigma <- matrix(c(1, 0.5, 0.5, 1), nrow = 2)
data = rGMM(n = 1e3, d = 3, k = 1, miss = 0.2)
fit <- FitGMM(data, k = 1)
show(fit)
```

Again, we see that the values of the `Estimated mean` vector are near-zero and those of the `Estimated covariance` matrix closely follow the identity matrix, as expected in this case.

## Cluster Number Selection

The function `ClustQual` provides several metrics for internally assessing the quality of cluster assignments from a fitted GMM (Arquez, 2020). The output is a list containing the following metrics:

* BIC (Bayesian information criterion): a penalized version of the negative log likelihood. A lower value indicates better clustering quality.

* CHI (Calinski-Harabaz index): a ratio of the between-cluster to within-cluster variation. A higher value indicates better clustering quality.

* DBI (Davies-Bouldin index): an average of similarities across clusters. A lower value indicates better clustering quality.

* SIL: average silhouette width, a measure of how well an observation matches its assigned cluster. A higher value indicates better clustering quality.

```{r}
set.seed(495)
mu <- list(
  c(2, 2),
  c(2, -2),
  c(-2, 2),
  c(-2, -2)
)
sigma <- 0.5 * diag(2)
data = rGMM(n = 100, d = 2, k = 4, means = mu, covs = sigma)
fit <- FitGMM(data, k = 4, maxit = 100, eps = 1e-8, report = F)

# Quality metrics
clust_qual = ClustQual(fit)
cat("BIC:\n")
clust_qual$BIC
cat("\nCHI:\n")
clust_qual$CHI
cat("\nDBI:\n")
clust_qual$DBI
cat("\nSIL:\n")
clust_qual$SIL
```

The biggest question in applying the GMM lies in the unknown number of clusters $k$. Using the inputs of the data matrix, the minimum number of clusters to assess `k0`, the maximum number of clusters to assess `k1`, and the number of bootstrap replicates at each cluster number boot, the function `ChooseK` provides guidance on the number of clusters likely present in the data [@McCaw2019.12.20.884551]. For each cluster number $k$ from `k0` to `k1`, boot bootstrapped data sets are generated. A GMM with $k$ components is fit, and the quality metrics from above are calculated. The bootstrap replicates are summarized by their mean and standard error (SE). Thus, for each quality metric, we obtain the cluster number `k_opt` that provided the optimal quality and the smallest cluster number whose quality was within 1 SE of the optimum `k_1se`.

```{r}
choose_k <- ChooseK(data, k0 = 2, k1 = 6, boot = 10)
cat("\nCluster number choices:\n")
choose_k$Choices
cat("\nAll results:\n")
head(choose_k$Results)
```


# Application to Data

```{r}
data(galaxies)
```

```{r}
# Multivariate Normal PDF Function Given a matrix x, mu (mean), and
# sigma (covariance), we can calculate the probability density for each
# row using the apply function. The function returns a column vector of
# probabilities.

max_iter <- 100
bic_values <- numeric(max_iter)

mvpdf <- function(x, mu, sigma) {
    if (det(sigma) == 0) {
        warning("Determinant is equal to 0.")
    }
    apply(x, 1, function(x) exp(-(1/2) * (t(x) - mu) %*% MASS::ginv(sigma) %*% 
        t(t(x) - mu))/sqrt(det(2 * pi * sigma)))
}

# Plot dataset
plot(iris[, 1:4], col = iris$Species, pch = 18, main = "Fisher's Iris Dataset")

# Mclust comes with a method of hierarchical clustering. We'll
# initialize 3 different classes.
initialk <- hc(data = iris, modelName = "EII")
initialk <- hclass(initialk, 3)
# First split by class and calculate column-means for each class.
mu <- split(iris[, 1:4], initialk)
mu <- t(sapply(mu, colMeans))
# Covariance Matrix for each initial class.
cov <- list(diag(4), diag(4), diag(4))
# Mixing Components
a <- runif(3)
a <- a/sum(a)

for (iter in 1:max_iter) {
  
# Calculate PDF with class means and covariances.
z <- cbind(mvpdf(x = iris[, 1:4], mu = mu[1, ], sigma = cov[[1]]), mvpdf(x = iris[, 
    1:4], mu = mu[2, ], sigma = cov[[2]]), mvpdf(x = iris[, 1:4], mu = mu[3, 
    ], sigma = cov[[3]]))

# Expectation Step for each class.
r <- cbind((a[1] * z[, 1])/rowSums(t((t(z) * a))), (a[2] * z[, 2])/rowSums(t((t(z) * 
    a))), (a[3] * z[, 3])/rowSums(t((t(z) * a))))

# Choose the highest rowwise probability
eK <- factor(apply(r, 1, which.max))

# Total Responsibility
mc <- colSums(r)

# Update Mixing Components.
a <- mc/NROW(iris)

# Update our Means
mu <- rbind(colSums(iris[, 1:4] * r[, 1]) * 1/mc[1], colSums(iris[, 1:4] * 
    r[, 2]) * 1/mc[2], colSums(iris[, 1:4] * r[, 3]) * 1/mc[3])

# Update Covariance matrix.
cov[[1]] <- t(r[, 1] * t(apply(iris[, 1:4], 1, function(x) x - mu[1, ]))) %*% 
    (r[, 1] * t(apply(iris[, 1:4], 1, function(x) x - mu[1, ]))) * 1/mc[1]

cov[[2]] <- t(r[, 2] * t(apply(iris[, 1:4], 1, function(x) x - mu[2, ]))) %*% 
    (r[, 2] * t(apply(iris[, 1:4], 1, function(x) x - mu[2, ]))) * 1/mc[2]

cov[[3]] <- t(r[, 3] * t(apply(iris[, 1:4], 1, function(x) x - mu[3, ]))) %*% 
    (r[, 3] * t(apply(iris[, 1:4], 1, function(x) x - mu[3, ]))) * 1/mc[3]

# Compute the sum of the mixture densities, take the log, and add the
# column vector.
loglik <- sum(log(apply(t(t(z) * a), 1, sum)))

# BIC is calculated using the equation
# bic <- -2 * loglik + 14 * log(NROW(iris))
bic_values[iter] <- -2 * loglik + 14 * log(NROW(iris))
}
```

```{r}
# After every iteration we can plot.
par(mfrow = c(1, 2))
plot(bic_values, type = "l", lwd = 2, col = "red", main = "BIC", xlab = "Iterations", 
    ylab = "Score")
plot(loglik, type = "l", lwd = 2, col = "blue", main = "Log-Likelihood", 
    xlab = "Iterations", ylab = "Score")

# Load the package
library(mclust)

# Select 4 continuous variables and look for three distinct groups.
mcl.model <- Mclust(iris[, 1:4], 3)

# Plot our results.
plot(mcl.model, what = "classification", main = "Mclust Classification")
```

# Conclusion
