---
title: "Homework 3 - Stat 495"
author: "Cassandra Jin"
date: "Due Monday, Oct. 2 by midnight"
output:
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---

```{r, include = FALSE}
library(mosaic)
library(ISLR)
library(GGally)
library(stats)
library(leaps)
library(ggrepel)
library(glmnet)
options(digits = 6)
```

# Practicing Academic Integrity

If you worked with others or used resources outside of provided course material (anything besides our textbook(s), course materials in Moodle/Git repo, R help menu) to complete this assignment, please acknowledge them below using a bulleted list. 

<!-- ~~~~~~~~~~~~~~~~ YOU MAY BEGIN EDITING BELOW THIS LINE ~~~~~~~~~~~~~~~~ -->

*I acknowledge the following individuals with whom I worked on this assignment:*

Name(s) and corresponding problem(s)

*

*I used the following sources to help complete this assignment:*

Source(s) and corresponding problem(s)

* 

\newpage

# PROBLEMS TO TURN IN: Additional 1-3

## Additional 1 - Applications to College

Adapted from ISLR

```{r}
data(College)
```

This data set contains information from 1995 on US Colleges from US News and World Report (see help file for details). Our goal is to predict the number of applications received (Apps) using the other variables as potential predictors. 

> part a: Split the data into training and test data sets. (2/3 - 1/3 split is fine.)

SOLUTION:

```{r}
College <- na.omit(College)

set.seed(495)
n <- nrow(College)
train_index <- sample(1:n, (2/3) * n)
test_index <- setdiff(1:n, train_index)

train <- College[train_index, ] 
test <- College[test_index, ]

#further set up for using glmnet
x_train <- model.matrix(Apps ~ ., train)[, -1]
x_test <- model.matrix(Apps ~ ., test)[, -1]

y_train <- train %>%
  select(Apps) %>%
  unlist() %>%
  as.numeric()

y_test <- test %>%
  select(Apps) %>%
  unlist() %>%
  as.numeric()
```

> part b: Fit a "kitchen sink" linear regression model on the training set. Report the test error obtained, and comment on any issues with the model (such as conditions, etc.).

SOLUTION:

```{r}
all_mod <- lm(Apps ~ ., data = train)
msummary(all_mod)
mplot(all_mod, which = 1)
mplot(all_mod, which = 2)
```

```{r}
all_pred <- predict(all_mod, newx = x_test, exact = T, x = x_train, y = y_train)
mean((all_pred - y_test)^2)
```

The test MSE is 26751721. The kitchen-sink model does not seem to satisfy the conditions well. We see extreme heteroscedasticity on the residuals vs. fitted plot, as well as a downward then plateauing trend from left to right. For the Q-Q plot, although many points lie solidly on the line for a center section, they begin to stray downward on the left and also upward on the right. These indicate that the model fails to satisfy the conditions of equal variances and normality of errors.

> part c: Fit a linear regression model using an automated variable selection method of your choice (from Stat 230) on the training set. Report the test error obtained, and comment on any issues with the model (such as conditions, etc.).

SOLUTION:

```{r}
best <- regsubsets(Apps ~ ., data = train, nbest = 1)
with(summary(best), data.frame(rsq, adjr2, cp, rss, outmat))

best_mod <- lm(Apps ~ Accept + Enroll + Top10perc + F.Undergrad + Outstate + PhD + Expend + Grad.Rate, data = College)
msummary(best_mod)
mplot(best_mod, which = 1)
mplot(best_mod, which = 2)
```

```{r}
best_pred <- predict(best_mod, newx = x_test, exact = T, x = x_train, y = y_train)
mean((best_pred - y_test)^2)
```

To maximize Adjusted $R^2$ and minimize Cp value, I picked the model with 8 predictors. The test MSE for this model is 25527700, slightly smaller than that of the kitchen-sink model. The best subsets model still does not seem to satisfy the conditions well. There is less of a shaped trend in the points on the residuals vs. fitted plot, but we still see extreme heteroscedasticity. For the Q-Q plot, the points deviate from the line further toward the ends compared to how they behave on the kitchen-sink Q-Q plot, but the points still stray considerably from the line. These indicate that the best subsets model also fails to satisfy the conditions of equal variances and normality of errors.

> part d: Fit a ridge regression model on the training set, with lambda chosen by cross-validation. Report the lambda chosen and the test error obtained.

SOLUTION:

```{r}
set.seed(495)
x <- model.matrix(Apps ~ ., College)[, -1] # trim off the first column
y <- College %>% select(Apps) %>%  unlist() %>%  as.numeric()

grid <- 10^seq(10, -2, length = 100)
ridge_mod <- glmnet(x, y, alpha = 0, lambda = grid)

dim(coef(ridge_mod))
# plot(ridge_mod, label = TRUE)

cvridge.out <- cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training data
bestlam_ridge <- cvridge.out$lambda.min  # Select lambda that minimizes training MSE
bestlam_ridge
```

```{r}
# fit model on training set
ridge_mod <- glmnet(x_train, y_train, alpha = 0, lambda = grid)
#get predictions on test set using model
ridge_pred <- predict(ridge_mod, s = bestlam_ridge, newx = x_test)
# compute MSE on test set
mean((ridge_pred - y_test)^2)
# view coefficients
predict(ridge_mod, type = "coefficients", s = bestlam_ridge)[1:18,]
```

The value of $\lambda$ that results in the smallest cross-validation error is 385.428. The test MSE for this model is 139216, which is much smaller than those of the kitchen-sink model and the best subsets model.

> part e: Fit a lasso model on the training set, with lambda chosen by cross-validation. Report the lambda chosen and the test error obtained.

SOLUTION:

```{r}
lasso_mod <- glmnet(x_train, y_train, alpha = 1, lambda = grid) #make sure alpha = 1 for Lasso
set.seed(495)
cvlasso.out <- cv.glmnet(x_train, y_train, alpha = 1)
# plot(cvlasso.out)
bestlam_lasso <- cvlasso.out$lambda.min
bestlam_lasso

lasso.pred <- predict(lasso_mod, s = bestlam_lasso, newx = x_test)
mean((lasso.pred - y_test)^2)
# plot(lasso_mod)
predict(lasso_mod, type = "coefficients", s = bestlam_lasso)[1:18,]
```

The value of $\lambda$ that results in the smallest cross-validation error is 1.87418, and the test MSE for this model is 1605308, which is larger than the test MSE for the ridge regression model.

> part f: Comment on the results obtained across the four models. Address the following questions as part of your response. Do the test errors differ much? Do the coefficients differ greatly? In particular, if any variables were left out of the model in (c) or (e), is there any insight that they might have been removed based on the models in (b) or (d)? Which final model would you select here? Why?

SOLUTION:

The kitchen-sink linear regression model and the model picked by the best subsets method had similarly huge test MSEs, within the same order of magnitude. The models obtained through ridge and lasso were also similar in test MSE, but the MSE values were much smaller than those of the kitchen-sink and best subsets linear regression models.

From the kitchen-sink model in (b) to the best subsets model in (c), the variables PrivateYes, Top25perc, P.Undergrad, Room.Board, Books, Personal, Terminal, S.F.Ratio, and perc.alumni were dropped, and all of these variables demonstrated either insignificant or nearly insignificant fitted coefficients in the summary output of the kitchen-sink model. With the best subsets method applied, some of the leftover variables (Accept, Enroll, F.Undergrad, Outstate, Expend, Grad.Rate) had coefficient values of smaller magnitude, while others (Top10perc, PhD) had larger magnitude in the model for (c).

Between the ridge and lasso models, no coefficients are eliminated. Both models don't set any coefficients to 0, but observing the variables which were determined to have significant predictability in the best subsets model, all of these variables had larger magnitudes in the lasso model than in the ridge. The only notable coefficient was the Enroll variable, which has a positive coefficient for ridge but a negative one for lasso.

\newpage

## Additional 2 - Soil predictions

The data comes from a Kaggle competition: https://www.kaggle.com/c/afsis-soil-properties. The original data set contained 3600 variables, 3599 possible predictors (really, 3578 and some other variables) and a response, Sand. The 3599 predictors were reduced to 106 (methods to be taught later this semester) that can "best" distinguish between the two levels of Depth (another variable in the data set). The resulting data set of 107 variables (106 predictors and the response variable, Sand) was saved in the data set "newsoil". The row numbers should be removed as demonstrated below.

```{r}
newsoil <- read.csv("https://awagaman.people.amherst.edu/stat495/newsoil.csv",
                    header = T)
newsoil <- select(newsoil, -X)
```

Our focus is on predicting the response variable Sand, using the selected variables from previous work. 

> part a: Split the data set into a training and test set (75/25), with a seed of your choice. You may also wish to create appropriate x and y matrices for future function inputs at the same time. 

SOLUTION:

```{r}
set.seed(495)
n <- nrow(newsoil)
train_index <- sample(1:n, 0.75 * n)
test_index <- setdiff(1:n, train_index)

train <- newsoil[train_index, ] 
test <- newsoil[test_index, ]

#further set up for using glmnet
x_train <- model.matrix(Sand ~ ., train)[, -1]
x_test <- model.matrix(Sand ~ ., test)[, -1]

y_train <- train %>%
  select(Sand) %>%
  unlist() %>%
  as.numeric()

y_test <- test %>%
  select(Sand) %>%
  unlist() %>%
  as.numeric()
```

> part b: Fit lasso models to predict Sand using all the possible predictors. Choose two lasso models - one that has a "best" lambda determined in some appropriate way, and another model with a different non-zero lambda of your choice. How many slope coefficients are set to 0 in each of your chosen lasso models?

SOLUTION:

```{r}
lasso_mod <- glmnet(x_train, y_train, alpha = 1, lambda = grid) #make sure alpha = 1 for Lasso
set.seed(495)
cvlasso.out <- cv.glmnet(x_train, y_train, alpha = 1)
# plot(cvlasso.out)
bestlam_lasso <- cvlasso.out$lambda.min
bestlam_lasso
```

```{r}
predict(lasso_mod, type = "coefficients", s = 6)[1:18,]
predict(lasso_mod, type = "coefficients", s = bestlam_lasso)[1:18,]
```

Using cross-validation to choose lambda, we find that 5.99889e-05 is the "best" lambda. In this model, all but one of the slope coefficients is set to 0, and in the model with my arbitrarily chosen lambda value of 6, all slope coefficients are 0.

> part c: Fit a ridge model to predict Sand using all the possible predictors. How many slope coefficients are set to 0 in your ridge model?

SOLUTION:

```{r}
set.seed(495)
x <- model.matrix(Sand ~ ., newsoil)[, -1] # trim off the first column
y <- newsoil %>% select(Sand) %>%  unlist() %>%  as.numeric()

grid <- 10^seq(10, -2, length = 100)
ridge_mod <- glmnet(x, y, alpha = 0, lambda = grid)

dim(coef(ridge_mod))
# plot(ridge_mod, label = TRUE)

cvridge.out <- cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training data
bestlam_ridge <- cvridge.out$lambda.min  # Select lambda that minimizes training MSE
bestlam_ridge
```


```{r}
# fit model on training set
ridge_mod <- glmnet(x_train, y_train, alpha = 0, lambda = grid)
# view coefficients
predict(ridge_mod, type = "coefficients", s = bestlam_ridge)[1:18,]
```

Fitting a ridge model to predict Sand using all the possible predictors, we find a best lambda value of 0.0599889, and none of the slope coefficients are set to 0 in this ridge model.

> part d: Compute test MSEs for both of your lasso models and your ridge model. 

SOLUTION:

```{r}
# arbitrary lasso MSE
lasso.pred1 <- predict(lasso_mod, s = 6, newx = x_test)
mean((lasso.pred1 - y_test)^2)

# selected lasso MSE
lasso.pred2 <- predict(lasso_mod, s = bestlam_lasso, newx = x_test)
mean((lasso.pred2 - y_test)^2)

# ridge MSE
ridge_pred <- predict(ridge_mod, s = bestlam_ridge, newx = x_test)
mean((ridge_pred - y_test)^2)
```

The test MSEs for the lasso model and the ridge model are 0.28289 and 0.278501, respectively.

> part e: Write a few sentences to address the following questions. Does the test MSE from the model with the "best" lambda suggest it is in fact a better predictive model than your other lasso model? Is ridge better than the lasso models? Which final model would you choose here from these three models? Why?

SOLUTION:

The test MSE from the model with the "best" lambda does suggest it is in fact a better predictive model than the other lasso model. I'm not sure how effectively large the magnitudes of the test MSEs for each lasso model are in the context of the data, but comparing the test MSE for the lasso model with arbitrary lambda (0.966146) and the test MSE for the lasso model with the cross-validated lambda (0.28289), we see that the latter is multiple times smaller than the former. However, the test MSE for the lasso model with the cross-validated lambda is still just slightly larger than that of the ridge model. Even so, the final model I would you choose here from these three is the lasso model with the cross-validated lambda, because the ridge model assigns non-zero slope coefficients to so many variables and thus retains a massive number of predictors, whereas the the lasso model yields many zero-value coefficients. For an increase of less than 0.005 of test MSE, I think it is worth it to use a much simpler model with fewer predictors allowed.

> part f: What is the default setting for the normalize option in lars and the standardize option in glmnet? Why is this setting important to the model fit?

SOLUTION:

The default setting for both the normalize option in lars and the standardize option in glmnet is TRUE. This is important to the model fit, because both normalization and standardization ensure that the regularization penalties are applied fairly and effectively to all predictors, regardless of their scales or magnitudes. Thus, they help maintain the interpretability and stability of the resulting models.

> part g: Explain what option you would change in order to fit an elastic net penalty (not OLS or ridge) using glmnet.

SOLUTION:

In order to fit an elastic net penalty (not OLS or ridge) using glmnet, I would adjust the alpha parameter to a value that represents the mixing parameter between lasso and ridge regression penalties. Since the function applies the lasso penalty when alpha = 1 and the ridge penalty when alpha = 0, I would choose an alpha value in between, such as 0.5, to balance the two penalties and achieve an elastic net one.

\newpage

## Additional 3 

After reading through your portfolio reflections, I decided to try to adapt some class activities and assignments to better align with your goals. This is a little bit challenging because they are quite varied. However, the *College* data set that we used for Additional 1 does permit a host of different analyses, so we're going to try using it to help with this.

For this problem, your assignment is to tackle some aspect of your goals for class using the *College* data set. Include your work here, in the outline below. I'll give you feedback and work to correct any statistical issues, and note that while assessing this, I'm not looking for any one specific thing from any of you. I'm including some examples of what you might do below, based on some of the goals I read. 

Examples, if you said ...

* I want to demonstrate my understanding of method X. Can you apply method X to this data set? (You may have to do some work like create a variable for example to run an ANOVA; take one of the quantitative variables and cut it into 4 groups.) Try it. Write a summary of your findings.
* I want to work on my EDA. We know the (overall) goal here is to predict Apps. What EDA would you do (we skipped it above!)? Describe it, do some of it, etc. 
* I want to practice commenting my code and making visuals extremely clear. Pick a simple analysis (predict Apps with like 2-3 other predictors), make your code awesome and visuals really good. 
* I want to practice writing more precisely / shorter paragraphs for my results. Pick a simple analysis (predict Apps with like 2-3 other predictors), and write your results the way you typically would. Then, leave that there, and try a revision, working to improve the writing (this way, you see the original and the revision). 

Other guidance:

* Spend at most 2 hours on this question. This is designed to let you practice something you wanted to work on, not take up all your time. 
* If none of your goals seem to align with this, you can pick one based on the examples I listed above, or something similar that you want to work on. 
* Use your best judgement for any models you need to fit here. If you just need a model to practice writing, it is okay if that's not the overall best model you might pick. On the other hand, if you want to practice model fitting, you should be spending time discussing that and showing your work for it. We will practice the entire data analysis process in future work. 

> part a: What aspect of your goals for class are you going to tackle for this assignment? How? 

SOLUTION:

I will try to perform EDA that is comprehensive but also efficient and tells me practical information to help narrow down my analysis paths. For the one binary variable, Private, there are many types of EDA that I could do, such as a frequency table, a stacked bar plot, or even a heatmap perhaps. As for the quantitative rest of the variables versus Apps, I can create histograms, scatterplots, or maybe density plots with overlapping distributions (overlay the density plots of the quantitative variable for each binary outcome to see the differences in distribution?) so as to see the datas' shape, spread, and center. For one of my class projects in STAT-320, my groupmates created violin plots, which I'd never seen before. I'd be interested in learning when that is useful as well. Lastly, I will consider including a correlation matrix among all of the variables to try to uncover instances of multicollinearity just on first glance. Also, although correlation is not a comprehensive statistic of the strength of an interaction between two variables, it may give a starting point to understanding the predictors relative to the response variables Apps and help to at least eliminate very weak ones.

> part b: Include your work here!

SOLUTION:


```{r}
p0 <- gf_dens(~ Apps, data = College, 
              ylab = "Density", 
              xlab = "Apps")
p0

College <- mutate(College, log_apps = log(Apps))
p1 <- gf_dens(~ log_apps, data = College, 
              ylab = "Density", 
              xlab = "log(Apps)")
p1

favstats(~ log_apps, data = College)
```

```{r echo = FALSE, warning = FALSE, fig.width = 3.5}
# Quantitative variables density plots initialization
p2 <- gf_dens(~ Accept, data = College, 
              ylab = "Density", 
              xlab = "Accepted")
p3 <- gf_dens(~ Enroll, data = College, 
              ylab = "Density", 
              xlab = "Enrolled")
p4 <- gf_dens(~ Top10perc, data = College, 
              ylab = "Density", 
              xlab = "Top10perc")
p5 <- gf_dens(~ Top25perc, data = College, 
              ylab = "Density", 
              xlab = "Top25perc")
p6 <- gf_dens(~ F.Undergrad, data = College, 
              ylab = "Density", 
              xlab = "F.Undergrad")
p7 <- gf_dens(~ P.Undergrad, data = College, 
              ylab = "Density", 
              xlab = "P.Undergrad")
p8 <- gf_dens(~ Outstate, data = College, 
              ylab = "Density", 
              xlab = "Outstate")
p9 <- gf_dens(~ Room.Board, data = College, 
              ylab = "Density", 
              xlab = "Room.Board")
p10 <- gf_dens(~ Books, data = College, 
              ylab = "Density", 
              xlab = "Books")
p11 <- gf_dens(~ Personal, data = College, 
              ylab = "Density", 
              xlab = "Personal")
p12 <- gf_dens(~ PhD, data = College, 
              ylab = "Density", 
              xlab = "PhD")
p13 <- gf_dens(~ Terminal, data = College, 
              ylab = "Density", 
              xlab = "Terminal")
p14 <- gf_dens(~ S.F.Ratio, data = College, 
              ylab = "Density", 
              xlab = "S.F.Ratio")
p15 <- gf_dens(~ perc.alumni, data = College, 
              ylab = "Density", 
              xlab = "perc.alumni")
p16 <- gf_dens(~ Expend, data = College, 
              ylab = "Density", 
              xlab = "Expend")
p17 <- gf_dens(~ Grad.Rate, data = College, 
              ylab = "Density", 
              xlab = "Grad.Rate")

# Qualitative variable bar graphs
p18 <- gf_bar(~ Private, data = College, 
             ylab = "Frequency",
             xlab = "Private")

p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13
p14
p15
p16
p17
p18
```

```{r}
# Statistical summary for some quantitative variables
favstats(~ Accept, data = College)
favstats(~ Enroll, data = College)
favstats(~ Top10perc, data = College)
favstats(~ F.Undergrad, data = College)

# Tally table for qualitative variable
tally(~ Private, format = "percent", data = College)
```


```{r message=FALSE}
ggpairs(College, columns = 2:ncol(College))
```

From the density plots of the potential quantitative predictors, we see that some variables are already mostly normally-distributed, e.g. Top25perc and Grad.Rate. Other variables, like Top10perc, Outstate, and S.F.Ratio are a bit skewed, while others, like Accepted and F.Undergrad are extremely right-skewed. These plots don't allow any conclusions about correlation between the predictors and the response variable, but they do suggest that different sorts of transformations on the predictors would be appropriate before regressing Apps on the them and proceeding with any analysis. Also the density plot of our desired response variable, Apps, demonstrates heavy right-skewness, which indicates that a transformation of the variable. After applying the log transform, we have a distribution much closer to normal.

