---
title: "Stat 495 - Chapter 15 Lab - Solution"
author: "A.S. Wagaman"
output:
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---


```{r, include = FALSE}
library(mosaic)
library(DescTools)
```

# ANOVA Example

We will use the current population survey data to do one example with One-Way ANOVA.

```{r}
data(CPS85)
```

We will look for differences in average log wage by sector.

```{r}
favstats(log(wage) ~ sector, data = CPS85)
gf_boxplot(log(wage) ~ sector, data = CPS85)
```

The log re-expression helps with satisfying appropriate conditions.

```{r}
CPS85 <- mutate(CPS85, logwage = log(wage))
mymod <- lm(logwage ~ sector, data = CPS85)
msummary(mymod) # the F and its p-value here is the same you'd get with the next line
anova(mymod)
mplot(mymod, which = 1)
mplot(mymod, which = 2)
```

What do we learn from the ANOVA output?

> SOLUTION

The plots tell us that the conditions appear to be met. Looking at the overall F-test, we see an F statistic of 16.73 with a tiny p-value. So, this suggests that at least one mean log(wage) is different from another somewhere within the possible pairwise comparisons. We just don't know where. 

To perform multiple comparisons, several functions can be used, but two common ones for this setting are pairwise.t.test and PostHocTest (in DescTools). 

```{r}
with(CPS85, pairwise.t.test(log(wage), sector, p.adjust = "bonf"))
PostHocTest(aov(log(wage)~ sector, data = CPS85), method = "bonf")
```

Compare the output between the methods for Bonferroni's approach. Which method do you prefer? Why?

> SOLUTION

The p-value table is easier to read for a large number of comparisons, but it doesn't provide confidence intervals. So, your preference might depend on what you want to be looking at. 

Here, we learn that 12 of the differences in mean log(wages) are significant at a level of 0.05.

Implement "none", "holm", and "fdr" methods for these comparisons using pairwise.t.test. What differences do you note in the results? (PostHocTests doesn't have all of these implemented for aov.)

> SOLUTION

```{r}
with(CPS85, pairwise.t.test(log(wage), sector, p.adjust = "none"))
with(CPS85, pairwise.t.test(log(wage), sector, p.adjust = "holm"))
with(CPS85, pairwise.t.test(log(wage), sector, p.adjust = "fdr"))
```

With no adjustment, we see very different results. A total of 16 differences are detected. The p-values are much lower than we saw with Bonferroni's method.

With holm's method, we see p-values in between Bonferroni's and no adjustment. The same 12 differences are found to be significant as previously identified with Bonferroni's. 

With the fdr method, the p-values are lower than Holm's but not as low as those with no adjustment. It looks like 14 differences would be denoted as interesting. 


# p.adjust methods

A suite of p.adjust methods exist - you've been calling them up above. All you need though is just a vector of p-values that can then be adjusted by these methods. As an example, consider:

```{r}
set.seed(495)
pvals <- sort(c(runif(100, 0, 0.01), runif(100, 0.01, 0.20))) #random potential p-values
adjpvals <- p.adjust(pvals, method = "bonf")
head(pvals)
head(adjpvals)
```

Here is the example from the help file:

```{r}
set.seed(123)
x <- rnorm(50, mean = c(rep(0, 25), rep(3, 25)))
p <- 2*pnorm(sort(-abs(x)))

round(p, 3)
round(p.adjust(p), 3)
round(p.adjust(p, "BH"), 3)
```

PostHocTest has some additional description of the methods used there in its help file.

```{r}
?PostHocTest
```


# Prostate Microarray example

The textbook uses the prostate microarray data example throughout the chapter.

The data set is provided on their website, but they also provide the already computed z-scores, from which p-values can be computed. 

```{r}
prostz <- read.table("http://web.stanford.edu/~hastie/CASI_files/DATA/prostz.txt", header = FALSE)
```

Examine the distribution of the z-scores. 

> SOLUTION

```{r}
gf_histogram(~ V1, data = prostz)
favstats(~ V1, data = prostz)
```


The textbook examines one-sided POSITIVE results, though in general you might care about just any non-null case. 

Consider two-sided hypotheses. How many null hypotheses would be rejected (out of the 6033) if the p-values were unadjusted and $\alpha = 0.05$? 

> SOLUTION

Here, we would want to split the 0.05 into the tails, equally, presumably. (You can split it unequally too). So, with 0.025 in each tail, the unadjusted z-score cutoff for a critical value is our familiar 1.96. So, we'd reject if Z was less than or equal to -1.96 or greater than or equal to 1.96. (The probability of it being exactly 1.96 in absolute value is 0, so this is ignored below.)

```{r}
qnorm(0.025)
length(which(abs(prostz$V1)>1.96))
```

We would identify 478 of the 6033 genes as interesting with this criterion. That's clearly way too large. That's almost 8% of the genes. 

What would you do differently if you wanted to apply Bonferroni's procedure to two-sided hypotheses?

> SOLUTION

The basic principle is the same - we split the alpha up over the m tests. But the cutoff we determine has to factor in the tails, as above. So we'd want the cutoff based on $\alpha/(2N)$. 


Perform your two-sided Bonferroni adjustment on the prostate z-scores by hand. How many null hypotheses are rejected at $\alpha = 0.05$?

> SOLUTION

```{r}
0.05/(2*6033)
cutoff <- 0.05/(2*6033)
-1*qnorm(cutoff)
```

The cutoff is 4.4576 (slightly larger than the book's 4.31 because we are looking at both tails, not just one.)

```{r}
length(which(abs(prostz$V1)>4.4576))
```

This flags only 3 of the z-scores as interesting, which seems like too few. 

Obtain the two-sided p-values (if you didn't up above - it can be done based on critical values alone).

> SOLUTION

```{r}
absz <- abs(prostz$V1)
pval2side <- 2*pnorm(absz, lower.tail = FALSE)
favstats(~ pval2side)
gf_density(~ pval2side)
length(which(pval2side <= 0.05)) # verify 478 were <= 0.05
```

Apply p.adjust using bonferroni to your p-values. Do you obtain the same results as you did by hand? How many null hypotheses are rejected at $\alpha = 0.05$?

> SOLUTION

```{r}
bonfpvals <- p.adjust(pval2side, method = "bonf")
length(which(bonfpvals <= 0.05))
```

Yes. These are the same results we obtained by hand. We identify only 3 p-values as interesting. 

Apply Holm's method using p.adjust. How many null hypotheses are rejected at $\alpha = 0.05$?

> SOLUTION

```{r}
holmpvals <- p.adjust(pval2side, method = "holm")
length(which(holmpvals <= 0.05))
```

This also gives us just 3 interesting results. 

Apply FDR using p.adjust. How many results are declared interesting at $q=\alpha=0.05$? At $q=0.1$?

> SOLUTION

```{r}
fdrpvals <- p.adjust(pval2side, method = "fdr")
length(which(fdrpvals <= 0.05))
length(which(fdrpvals <= 0.1))
```

This gives us 21 interesting results at alpha = 0.05, and 60 if we go up to 0.1 as the cutoff. 

How do the methods compare?

> SOLUTION

The fdr method is identifying more interesting results. Holm's and Bonferroni behaved very similarly. 

# Khan Data

The Khan dataset contains 2308 rows (genes) x 64 column (observations). Khan et al., 2001 used cDNA microarrays containing 6567 clones of which 3789 were known genes and 2778 were ESTs to study the expression of genes in of four types of small round blue cell tumours of childhood (SRBCT). These were neuroblastoma (NB),rhabdomyosarcoma (RMS), Burkitt lymphoma, a subset of non-Hodgkin lymphoma (BL), and the Ewing family of tumours (EWS). Gene expression profiles from both tumour biopsy and cell line samples were obtained and are contained in this dataset. The dataset downloaded from the website contained the filtered dataset of 2308 gene expression profiles as described by Khan et al., 2001.

```{r}
khan <- read.csv("https://awagaman.people.amherst.edu/stat495/khan_train.csv", header = T)
khan <- khan[, -1] #remove row indices
khant <- data.frame(t(khan)) #transpose so rows become columns
```

We want to identify the top genes that allow us to significantly distinguish between tumor types, appropriately adjusting for simultaneous testing. Each ROW in the initial data set should be used as the response in ANOVA (which is why we flipped it above to put those as the columns in our usual way of thinking about data sets), where the class labels are:

```{r}
labels <- c(rep("EWS", 23), rep("BL", 8), rep("NB", 12), rep("RMS", 21))
```

Perform appropriate ANOVA analyses, adjusting for simultaneous testing. An example way to extract the p-value for the *first* gene is shown below to help get you started.

```{r}
anova(lm(khant[, 1]~labels))$"Pr(>F)"[1]
```

> SOLUTION

We need to obtain the ANOVA p-values so that we can then adjust them. We know how to extract the p-value for a single gene, so we really just need a loop (or some apply function) and to store the resulting values. 

```{r}
anovapvals <- rep(0, 2308)

for(i in 1:2308){
  anovapvals[i] <- anova(lm(khant[, i]~labels))$"Pr(>F)"[1]
}
```

Now we have 2308 p-values! We can examine their distribution.

```{r}
favstats(~ anovapvals)
gf_histogram(~ anovapvals)
```

There are a LOT of small p-values.

```{r}
length(which(anovapvals <= 0.05))
length(which(anovapvals <= 0.01))
```


We need to adjust for doing 2308 tests simultaneously. As an example, I'll run fdr and holm's, but I am also dropping the overall alpha to 0.01 because this is a medical study. 

```{r}
fdranovapvals <- p.adjust(anovapvals, method = "fdr")
length(which(fdranovapvals <= 0.01))
length(which(fdranovapvals <= 0.1)) #just to see
```

```{r}
holmanovapvals <- p.adjust(anovapvals, method = "holm")
length(which(holmanovapvals <= 0.01))
```

These methods identify a LOT of interesting results. Even being conservative with holms and using an overall alpha of 0.01, we identify 178 interesting results out of 2308. The fdr method identifies a lot more. 

