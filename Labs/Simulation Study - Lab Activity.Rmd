---
title: "Simulation Study - Lab Activity"
output:
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---


```{r, include = FALSE}
library(mosaic)
library(broom)
library(knitr)
require(BSDA) #may need to install; used for Sign Test
options(digits = 6)
```

This lab is designed to have you practice coding up / writing simulation studies, and explore commenting code and other best practices for writing R code (the lessons apply to most other coding languages).

# Best Practices for Coding in R

Basically, I've just assembled some resources for you to examine at your leisure. There are many other similar pages out there. 

[https://www.r-bloggers.com/r-code-best-practices/](https://www.r-bloggers.com/r-code-best-practices/)

This first link has lots of links to other resources at the bottom of it's page. 

[Example Best Practices](https://swcarpentry.github.io/r-novice-inflammation/06-best-practices-R/)

[Google's Style Guide](https://google.github.io/styleguide/Rguide.xml)

In regards to commenting, you don't need to comment EVERY line, but the idea is to comment chunks, functions, etc. with their use/purpose. This helps both you and your readers. You also want to keep chunks relatively small - they become hard to parse when they get large (like paragraphs). Don't try to do too much in each chunk. 

I strongly recommend the following when it comes to your course projects (and other future work) in terms of coding, especially if you are running a simulation.

* Compile early and often. Don't wait till the last minute to compile.
* Write code in small chunks to check that it works. A large chunk that doesn't run is harder to debug.
* Intentionally break code INTO small chunks. You can always put it into a larger chunk later if needed. 
* Caching is useful, but be careful about pushing/committing said pieces to your repos. The limit is 100MB. 
* Learn RMarkdown formatting and use it. There's a cheatsheet from RStudio. 
* Don't submit at the last minute. If it's late, it's late. Git gives us a time record! Submit EARLY.
* Make sure you set seeds. You need a reproducible submission for the project.

# Simulation Uses - Generate Toy Examples and Simulation Studies

Simulation can be used in many ways that are useful for your audience. For the paper, you are choosing between an application and a simulation study, so we want to explore what a simulation study is (and isn't).

You may use simulation to generate a toy data set or example to illustrate a point in your exposition. For example, this is what I did in the Ch. 15 activity you explored. I generated 100 p-values from a particular setup so we could explore them, and know the truth while we did so. This is a toy example. 

A simulation study kicks this up a notch. Here, the idea is you have something you want to show or do (such as compare two methods), and you know that just doing it on a single simulated data set is nice but that's just a single ``data point''. You need to replicate. 

Here are two examples of simulation studies from my own work:

A colleague and I have a nonparametric test for interaction in the two-way layout (ANOVA). To show how it performs against competitors, we

* identified multiple competitors we wanted to check,
* chose two main forms of interaction we wanted to study,
* chose different effect sizes for the two factors involved, 
* picked different distributions for the errors in the models, and
* picked different sized layouts to use. 

At EACH combination of those, we generated multiple data sets and computed the power for each competing test procedure. 

My thesis advisor and I developed a new covariance estimator to use in high-dimensional settings. To show how it performed against competitors, we

* identified multiple competitors we wanted to check,
* chose two main covariance structures we wanted to study,
* chose different strengths of relationships in the covariance matrices to use, and
* picked three values for p, the number of variables involved.

At EACH combination of those, we generated covariance matrices, applied the estimators, and got norms comparing the estimates to the truth (which we knew because that's how we generated the matrices).

If you are aiming to do a simulation study for your project, I encourage you to think about what you want to show, and only choose a few things to have different levels of to compare. For example, perhaps you apply 2 different procedures on data sets generated with 3 different percentages of missingness, using some underlying common model with a fixed number of variables involved. 

## Simulation Study - Example

What things/quantities might you want to study in a simulation study? You could check things like Type I error, power, or confidence interval coverage. The simulation study below is in the setting of nonparametric statistics. It compares three test procedures in the one-sample setting examining measures of center - the parametric t-test, which you should know, and 2 nonparametric tests (the sign test and signed rank test).

From theory, we know that there are situations where the t-test does not perform well compared to the Sign Test or Signed Rank test, and vice versa. To investigate and demonstrate this to ourselves, we'll be setting up a simulation study, and walking through the steps associated with that below. 

What do we need to do in order to set up this simulation study? 

* We need to know how to run the 3 tests on a single data set.
* We need to figure out how to generate data sets under different conditions to study behavior.
* We need to know what conditions we're using and any relevant info for each. 
* We need to figure out what to save from each application of each test on each data set in order to demonstrate our point from the study. 


If you are doing a simulation study for the project, feel free to ask for assistance with this part - the setup can be the most challenging component. 

## Running the three tests

First, let's just see how to run all three test procedures on a set of observations. You can imagine these are the differences or just a set of values (i.e. paired setting or just one-sample setting). All these are run as two-sided examples. That could be changed with the *alternative* option, but we'll run everything here two-sided. 

```{r}
fakedata <- c(1:50) #1:50 as our data
t.test(~ fakedata, mu = 25) #parametric t-test
SIGN.test(fakedata, md = 25) #sign test
wilcox.test(fakedata, mu = 25) #signed rank test
```

Now, to do our simulation study and compare the methods, we're going to need to *SAVE* some values from the output of the tests. What might be useful values to save? (Last item above in list.)

> THINK about this before continuing on. 










It would probably be useful to save the test statistics and p-values (primarily p-values). Let's consider how that can be done. We can rerun the tests and SAVE the output in order to see what pieces are actually computed by R when the test is run. These pieces can be extracted separately.

```{r}
tresult <- t.test(~ fakedata, mu = 25) #parametric t-test
names(tresult)
signresult <- SIGN.test(fakedata, md = 25) #sign test
names(signresult)
srresult <- wilcox.test(fakedata, mu = 25) #signed rank test
names(srresult)
```

So, this means that I could extract all three p-values from the saved tests like this:

```{r}
tresult$p.value
signresult$p.value
srresult$p.value
```

(You should be able to do this for similar pieces of similar objects.)

Even better, I don't have to SAVE the test output to get the p-values. I can get them like this:

```{r}
t.test(~ fakedata, mu = 25)$p.value 
SIGN.test(fakedata, md = 25)$p.value 
wilcox.test(fakedata, mu = 25)$p.value 
```

Ok. So now we have a way to extract p-values quickly from a test. We want to combine this with simulating MANY data sets, using different distributions for the data to see how the different tests behave. (There's a bit more to this, including understanding effect size, but we should be able to have some fun with just this understanding.)

## Simulating Data from a Normal Distribution 

Here, we look at an example simulating data from the normal distribution - this is a condition we could say we want to explore. The code below samples 10 observations from a normal distribution with mean 20 and sd 2, and saves them as x. 

```{r}
x <- rnorm(10, 20, 2); x
```

We could feed x into our different tests, along with a hypothesized mean, get the p-values, and compare the results. For the study, we need to simulate MANY different but similar x's (i.e. same ``condition'') and repeat this process to better understand the behavior of the tests.

Here's an example:

```{r}
# Set Useful Values
reps <- 1000 #number of repetitions
truemu <- 40 
truesd <- 3
# if truemu and testcenter are not =, we hope tests detect this
testcenter <- 37 #center to test for
n <- 25 #sample size
set.seed(1001) #for reproducibility

#Initialize storage vectors
tpvals <- rep(0, reps)
spvals <- rep(0, reps)
srpvals <- rep(0, reps)

#Generate random data, do tests, save p-values
for(i in 1:reps){
  x<-rnorm(n, truemu, truesd)
  tpvals[i] <- t.test(~x, mu = testcenter)$p.value 
  spvals[i] <- SIGN.test(x, md = testcenter)$p.value 
  srpvals[i] <- wilcox.test(x, mu = testcenter)$p.value 
}
```

What does running this give us? Well, we get a set of 1000 p-values each from the three different tests (1000 data sets all run through the three tests). We can figure out how often each test rejected the null, and see which test is performing better. Here, we know the true mean is 40, and we were testing a two-sided alternative with a mean of 37. So, we'd like to think the tests would reject the null.  

```{r}
sum(tpvals <= 0.05)/reps
sum(spvals <= 0.05)/reps
sum(srpvals <= 0.05)/reps
```

We see that 99.9% of t-tests and Signed rank tests rejected the null, but only 95.6% of sign tests did. The output is the fraction of p-values less than or equal to 0.05.


Now you try. Here is the same code (pieces combined), and turned into a function (yes, you will probably want functions if doing your own simulation study, but you may need different functions for your different conditions, depending on how complicated it is). You run the main chunk once to create the function, and then you can run it quickly with different inputs in chunks after it.

```{r}
simnormal <- function(locationinput, scaleinput, testcenterinput, ninput, repsinput = 1000){
# Set Useful Values
reps <- repsinput #number of repetitions
location <- locationinput 
scale <- scaleinput
testcenter <- testcenterinput #center to test for
n <- ninput #sample size

#Initialize storage vectors
tpvals <- rep(0,reps)
spvals <- rep(0,reps)
srpvals <- rep(0,reps)

#Generate random data, do tests, save p-values
for(i in 1:reps){
  x <- rnorm(n, location, scale)
  tpvals[i] <- t.test(~x, mu = testcenter)$p.value 
  spvals[i] <- SIGN.test(x, md = testcenter)$p.value 
  srpvals[i] <- wilcox.test(x, mu = testcenter)$p.value 
}

output <- c(sum(tpvals <= 0.05)/reps, sum(spvals <= 0.05)/reps, sum(srpvals <= 0.05)/reps)
names(output) <- c("Ttest", "Sign", "SignedRank")
output
}
```

In the code chunk below, change the inputs a few times and see what sorts of results you get. The output is in the order of t-test, sign test, and lastly, signed rank test, and it produces the fractions of p-values less than or equal to 0.05. Some short labels were added to help with keeping the order straight. You could envision fancier functions that took in optional alphas to compare to instead of just 0.05. But this works for our purposes. 

```{r}
set.seed(1001)
simnormal(40, 3, 37, 25) #mean, sd, testcenter, n are the inputs
#simnormal()
#simnormal() #add more lines for more settings
```

What tests seem to be performing best based on your simulations?

> SOLUTION

In this setting, the data we generated followed a normal distribution, so we expected the t-test to do well, but signed rank should have been nearly comparable, according to theory. Let's try some different distributions - the t-test assumes a normal distribution, so if we change it, can the nonparametric methods do better than the t-test? (The answer is yes, by the way.) 



## Your Turn

Continue in the nonparametric comparison setting but change the distribution to something like a gamma, cauchy, or uniform instead of normal to see how the tests perform. Besides changing the distribution, you could envision changing effect sizes (how close the true center is to the center you are telling the tests to look at), or the effect of sample size, or the effect of spread. 

```{r}
simnormal_cauchy <- function(locationinput, scaleinput, testcenterinput, ninput, repsinput = 1000){
# Set Useful Values
reps <- repsinput #number of repetitions
location <- locationinput 
scale <- scaleinput
testcenter <- testcenterinput #center to test for
n <- ninput #sample size

#Initialize storage vectors
tpvals <- rep(0,reps)
spvals <- rep(0,reps)
srpvals <- rep(0,reps)

#Generate random data, do tests, save p-values
for(i in 1:reps){
  x <- rcauchy(n, location, scale)
  tpvals[i] <- t.test(~x, mu = testcenter)$p.value 
  spvals[i] <- SIGN.test(x, md = testcenter)$p.value 
  srpvals[i] <- wilcox.test(x, mu = testcenter)$p.value 
}

output <- c(sum(tpvals <= 0.05)/reps, sum(spvals <= 0.05)/reps, sum(srpvals <= 0.05)/reps)
names(output) <- c("Ttest", "Sign", "SignedRank")
output
}
```

## Presenting simulation study results

Presenting results from a simulation study usually means making a table (or several) or figure (or several) to display results, which you then have to discuss. Figuring out the best layout of tables is often tricky, and I encourage you to sketch it (by hand!) to figure out what makes the best sense for your setting. 
