---
title: "Stat 495 - Regularization Lab - Ridge and LASSO - Solution"
author: "A.S. Wagaman"
output:
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---


```{r, include = FALSE}
library(mosaic)
library(broom)
library(glmnet)
library(ISLR)
library(lars)    # you may need to install some of these (one time only)
library(leaps)   # to use variable selection methods if desired
```

## Ridge Regression

This first example is adapted from ISLR, pages 251-255, from the reproduction of that lab by McNamara and Crouser.  

```{r}
data(Hitters)
# ?Hitters # MLB data from 1986 and 1987; 322 obs on 20 variables
```

We want to predict Salary, which is missing for 59 players. The ridge regression function won't automatically remove NAs, so we do it.

```{r}
Hitters <- na.omit(Hitters) #263 obs now
```

We demo the new functions and code below. Ideally, we'd split into a training/test data set here, but we'll do that below after checking out these functions. 

The function we'll be using is *glmnet* which can do more than just ridge regression, or *cv.glmnet* for cross-validation. 

The format these functions require is not the usual Y ~ X format we are used to. Instead, we need to pass in the X matrix, and Y vector. 

```{r}
x <- model.matrix(Salary ~ ., Hitters)[, -1] # trim off the first column
                                         
y <- Hitters %>% select(Salary) %>%  unlist() %>%  as.numeric()
```

The `model.matrix()` function is particularly useful for creating X; not only does it produce a matrix corresponding to the 19 predictors but it also automatically transforms any qualitative variables into dummy variables as shown below (if you aren't sure what this means, please ask). The latter property is important because `glmnet()` can only take numerical, quantitative inputs.


```{r}
head(Hitters)
head(x)
```


## Fitting the Ridge Model via an Example

*glmnet* can be used to fit several types of models, including both ridge and LASSO, so there is an argument to the function governing what type of model is fit. That parameter is $\alpha$, and you want $\alpha=0$ for ridge regression, so you will see that below. 

Most of the text and code for the rest of this example is taken from the McNamara and Crouser reproduced lab, though it has been edited to change assignment operators to <- instead of =, and other similar changes/extra comments.

```{r}
grid <- 10^seq(10, -2, length = 100)
ridge_mod <- glmnet(x, y, alpha = 0, lambda = grid)
```

By default the `glmnet()` function performs ridge regression for an automatically
selected range of $\lambda$ values. However, here we have chosen to implement
the function over a grid of values ranging from $\lambda = 10^{10}$ to $\lambda = 10^{-2}$, essentially covering the full range of scenarios from the null model containing
only the intercept, to the least squares fit. 

(Note: the function has a pretty good automatic range of $\lambda$ values, so you can also just let it use the default. This is designed to show you how you could specify these values if you wanted.)

As we will see, we can also compute
model fits for a particular value of $\lambda$ that is not one of the original
grid values. Note that by default, the `glmnet()` function standardizes the
variables so that they are on the same scale. To turn off this default setting,
use the argument `standardize = FALSE`. 

(Note: this can have severe implications for your coefficient estimates, which is why TRUE is the default.)

Associated with each value of $\lambda$ is a vector of ridge regression coefficients,
stored in a matrix that can be accessed by `coef()`. In this case, it is a $20 \times 100$
matrix, with 20 rows (one for each predictor, plus an intercept) and 100
columns (one for each value of $\lambda$).

We can look at the path of the solution with a plot. Note that the plot has the $l_1$ norm (for the sum of the betas) on the x-axis, not the value of $\lambda$. $\lambda = 0$ is the OLS estimates on the far right, while $\lambda = \infty$ is the $l_1$ norm of 0 on the far left. 

```{r}
dim(coef(ridge_mod))
plot(ridge_mod, label = TRUE)    # Draw plot of coefficients; can label them by variable #
```

We expect the coefficient estimates to be much smaller, in terms of $l_2$ norm,
when a large value of $\lambda$ is used, as compared to when a small value of $\lambda$ is
used. These are the coefficients when $\lambda = 11497.57$, along with their $l_2$ norm:

```{r}
ridge_mod$lambda[50] #Display 50th lambda value
coef(ridge_mod)[, 50] # Display coefficients associated with 50th lambda value
sqrt(sum(coef(ridge_mod)[-1, 50]^2)) # Calculate l2 norm
```

In contrast, here are the coefficients when $\lambda = 705.4802$, along with their $l_2$
norm. Note the much larger $l_2$ norm of the coefficients associated with this
smaller value of $\lambda$.

```{r}
ridge_mod$lambda[60] #Display 60th lambda value
coef(ridge_mod)[, 60] # Display coefficients associated with 60th lambda value
sqrt(sum(coef(ridge_mod)[-1, 60]^2)) # Calculate l2 norm
```

We can use the `predict()` function for a number of purposes. For instance,
we can obtain the ridge regression coefficients for a new value of $\lambda$, say 50:

```{r}
round(predict(ridge_mod, s = 50, type = "coefficients")[1:20, ], 3)
```

We now split the samples into a training set and a test set in order
to estimate the test error of ridge regression. This uses a 50-50 split, although splits such as 80-20 or 70-30 are more common. There are multiple ways to do this.  

```{r}
set.seed(1) #set in ISLR lab
n <- nrow(Hitters)
train_index <- sample(1:n, 0.5 * n)
test_index <- setdiff(1:n, train_index)

train <- Hitters[train_index, ] 
test <- Hitters[test_index, ]

#further set up for using glmnet
x_train <- model.matrix(Salary ~ ., train)[, -1]
x_test <- model.matrix(Salary ~ ., test)[, -1]

y_train <- train %>%
  select(Salary) %>%
  unlist() %>%
  as.numeric()

y_test <- test %>%
  select(Salary) %>%
  unlist() %>%
  as.numeric()
```

Getting the data into the format for glmnet is most of the code here. If you were just running *lm* or *glm*, you would be fine using the train and test data sets constructed.  

Next we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using $\lambda = 4$, as an example. There is no reason to think this is a "good" lambda - it's just for illustration. Note the use of the `predict()` function again: this time we get predictions for a test set, by replacing `type="coefficients"` with the `newx` argument.

(Note: Be careful with arguments, for some predict functions, you need *newx* and others are *newdata*, etc. Use the help files to assist you.)

```{r}
# fit model on training set
ridge_mod <- glmnet(x_train, y_train, alpha = 0, lambda = grid, thresh = 1e-12)
#get predictions on test set using model
ridge_pred <- predict(ridge_mod, s = 4, newx = x_test)
# compute MSE on test set
mean((ridge_pred - y_test)^2)
```

The test MSE is 142199.2. Note that if we had instead simply fit a model
with just an intercept, we would have predicted each test observation using
the mean of the training observations. In that case, we could compute the
test set MSE like this:

```{r}
mean((mean(y_train) - y_test)^2)
```

Comparing these MSEs tells us that the ridge regression is doing better than a model with no predictors. 

(If you have any questions about this comparison, please ask for assistance.)

To compare MSEs, we could also get MSE for a model with just an intercept by fitting a ridge regression model with a very large value of $\lambda$. Note that `1e10` means $10^{10}$.

```{r}
# we already fit the set of models
# just get predictions with our chosen lambda
ridge_pred <- predict(ridge_mod, s = 1e10, newx = x_test)
# then compute MSE
mean((ridge_pred - y_test)^2)
```

So fitting a ridge regression model with $\lambda = 4$ leads to a much lower test
MSE than fitting a model with just an intercept. We now check whether
there is any benefit to performing ridge regression with $\lambda = 4$ instead of
just performing least squares regression. Recall that least squares is simply
ridge regression with $\lambda = 0$.

Note: In order for `glmnet()` to yield the exact least squares coefficients when $\lambda = 0$,
we use the argument `exact=T` when calling the `predict()` function. Otherwise, the
`predict()` function will interpolate over the grid of $\lambda$ values used in fitting the
`glmnet()` model, yielding approximate results. Even when we use `exact = T`, there remains
a slight discrepancy in the third decimal place between the output of `glmnet()` when
$\lambda = 0$ and the output of `lm()`; this is due to numerical approximation on the part of
`glmnet()`.


```{r}
#compare coefficients first
lm(Salary ~ ., data = train)
predict(ridge_mod, s = 0, exact = T,  x = x_train, y = y_train, type = "coefficients")[1:20, ]

#this code required some editing due to changes in the functions. The training data had to be added as arguments.
ridge_pred <- predict(ridge_mod, s = 0, newx = x_test, exact = T, x = x_train, y = y_train)
mean((ridge_pred - y_test)^2)
```

It looks like we are indeed improving over regular least-squares! 

> What two values are being compared to argue this?

The test set MSE for the lm and for the ridge regression are being compared. The values are 168588.6 for the lm and 142199.2 for the ridge regression with $\lambda = 4$.


Side note: in general, if we want to fit a (unpenalized) least squares model, then we should use the `lm()` function, since that function provides more useful outputs, such as standard errors and $p$-values for the coefficients.

Instead of arbitrarily choosing $\lambda = 4$, it would be better to
use cross-validation to choose the tuning parameter $\lambda$. We can do this using
the built-in cross-validation function, `cv.glmnet()`. By default, the function
performs 10-fold cross-validation, though this can be changed using the
argument `folds`. Note that we set a random seed first so our results will be
reproducible, since the choice of the cross-validation folds is random.


```{r}
set.seed(1)
cv.out <- cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training data
bestlam <- cv.out$lambda.min  # Select lambda that minimizes training MSE
bestlam
```

Therefore, we see that the value of $\lambda$ that results in the smallest cross-validation
error is 326.0828. We can also plot the MSE as a function of log($\lambda$):


```{r}
plot(cv.out) # Draw plot of training MSE as a function of log(lambda)
```

Now, we want to find out what the test MSE associated with this value of
$\lambda$ is.

```{r}
ridge_pred <- predict(ridge_mod, s = bestlam, newx = x_test) # Use best lambda to predict test data
mean((ridge_pred - y_test)^2) # Calculate test MSE
```

This represents a further improvement over the test MSE that we got using
$\lambda = 4$. Finally, we examine the coefficient estimates for our ridge regression on the training data set, using the best lambda we selected via cross-validation. You can do this by running predict on our cv.glmnet object or re-fitting the model using glmnet. 

```{r}
# uses cv.glmnet object
predict(cv.out, type = "coefficients", s = bestlam)[1:20,]

#re-fits on training first
out <- glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training dataset
predict(out, type = "coefficients", s = bestlam)[1:20,] # Display coefficients using lambda chosen by CV
```

As expected, none of the coefficients are exactly zero - ridge regression does not
perform variable selection!  

Important: The presentation of the functions and order of analysis here does not match what you would typically do in practice. It was designed to introduce you to the functions and how they work. What do you think the normal order of operations is? For example, would you spend a lot of time looking at coefficients for lambda values you have chosen at random? 

Here is a better sequence of analysis tasks:

* Determine what analysis you want to perform and how the data needs to be set up for that.
* Create a train/test split if you are doing predictive modeling and want to assess performance using the test set.
* Fit the model on the training set, using appropriate functions to set values for tuning parameters.
* Assess performance on the test set (as appropriate). 


## Example Lasso Fitting

Lasso stands for least absolute shrinkage and selection operator and was developed by Tibshirani.

We can fit these models with either the lars package or glmnet. In glmnet, the only change from the ridge code above is that you have to set the alpha to 1 to run LASSO. 

```{r, message=FALSE}
data(diabetes)
head(diabetes, 2)
```

We demo the lasso code on the same data set used in the original LARS paper (2002) with the lars package. Note the setup (you can check the help file for details) uses a matrix of predictors and then the response as we saw before with glmnet. 

```{r}
object1 <- with(diabetes, lars(x, y, type = "lasso"))
plot(object1)
coef(object1)
summary(object1)
```

The type you set has implications for the solution. The lasso option is the default. But you can also get the lar (without the lasso modification for if a nonzero coefficient hits zero) solution and forward stagewise solution (forward but in very small steps) via the lars algorithm (not quite forward selection).  

```{r}
object2 <- with(diabetes, lars(x, y, type = "lar"))
summary(object2)
plot(object2)
object3 <- with(diabetes, lars(x, y, type = "forward.stagewise"))
summary(object3)
plot(object3)
```

What differences do you see in the solutions here based on type? Which models would you pick for each option of "type"? How do those models differ?


## Your turn - Boston crime

Now that you've seen how to fit ridge and lasso models, let's try it out on another data set. Work in groups of 2 or 3 to help each other and discuss your results.

Your goal is to predict per capita crime rate in the Boston data set using ridge regression, lasso, and MLR model(s) of your choosing. How do the models compare? Which do you prefer? Feel free to look up the help file on the data set for more information about the variables. 

```{r}
Boston <- MASS::Boston #do NOT load the MASS library - it causes conflicts with dplyr
names(Boston)
```

a) Start by fitting and comparing ridge and LASSO regressions. The Ridge regression (chapter 7 for code) has been fit for you, with a training data set of 50% of the observations. 

```{r}
set.seed(495)
xBos <- model.matrix(crim ~ ., Boston)[ , -1]  
yBos <- Boston$crim
grid <- 10^seq(10, -2, length = 100)

n <- nrow(Boston)
train_index <- sample(1:n, 0.5 * n)
test_index <- setdiff(1:n, train_index)
trainBos <- Boston[train_index, ] 
testBos <- Boston[test_index, ]
yBos.test <- yBos[test_index]
```

That was the data setup for the glmnet function. Here is the ridge fit with cross-validation used to choose the tuning parameter. 

```{r}
ridgeBos.mod <- glmnet(xBos[train_index,], yBos[train_index], alpha = 0, lambda = grid)
set.seed(495)
cvBos.out <- cv.glmnet(xBos[train_index,], yBos[train_index], alpha = 0)
plot(cvBos.out)
bestlamBos <- cvBos.out$lambda.min
bestlamBos
```

Once we have the best lambda chosen by CV, we can look at the test MSE, etc. 

```{r}
ridgeBos.pred <- predict(ridgeBos.mod, s = bestlamBos, newx = xBos[test_index,])
mean((ridgeBos.pred - yBos.test)^2)
plot(ridgeBos.mod)
#coef(ridgeBos.mod) #can see coefs for all lambdas...
predict(ridgeBos.mod, type = "coefficients", s = bestlamBos)[1:14,]
```

Now you should fit the lasso model. Try fitting it via both the glmnet (change the alpha!) and lars functions.

```{r}
# Example using glmnet
lassoBos.mod <- glmnet(xBos[train_index,], yBos[train_index], alpha = 1, lambda = grid) #make sure alpha = 1 for Lasso
set.seed(495)
cvlBos.out <- cv.glmnet(xBos[train_index,], yBos[train_index], alpha = 1)
plot(cvlBos.out)
bestlamBosl <- cvlBos.out$lambda.min
bestlamBosl

lassoBos.pred <- predict(lassoBos.mod, s = bestlamBosl, newx = xBos[test_index,])
mean((lassoBos.pred - yBos.test)^2)
plot(lassoBos.mod)
#coef(lassoBos.mod) #can see coefs for all lambdas...
predict(lassoBos.mod, type = "coefficients", s = bestlamBosl)[1:14,]

```


What differences do you see in the models? Between the ridge and lasso fits?

Interestingly, the lasso sets several coefs to 0 - indus, nox, rm, age, and tax, with the magnitudes of others are very close to 0.  Most coef signs agree between the two except for those now set to 0. The rad and ptratio coefficients are larger in magnitude in the lasso solution than the ridge solution. The biggest difference is the number of predictors removed by setting coefs to 0. 

Now we fit this with the lars function.

```{r}
object1 <- lars(xBos[train_index,], yBos[train_index], type = "lasso")
plot(object1)
coef(object1)
summary(object1)
```

It looks like the 9 labeled model (row 8, df = 9) is similar to what we got with the best lambda approach with the glmnet function. However, based on Cp values, we might choose the 5th labeled model (df= 5, lowest Cp) which has even more coefs set to 0. 

```{r}
summary(object1)[9,]
coef(object1)[9,]

summary(object1)[5,]
coef(object1)[5,]
```

We could change the type argument and see the lar solution to see if that differs:

```{r}
object2 <- lars(xBos[train_index,], yBos[train_index], type = "lar")
plot(object2)
coef(object2)
summary(object2)
```

Doesn't appear to differ here.

```{r}
summary(object2)[5,]
coef(object2)[5,]
```


b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, cross- validation, or some other reasonable alternative, as opposed to using training error.

SOLUTION: 

The test MSE for ridge is 37.0696, with a lambda chosen by CV of roughly 0.92.

The test MSE for lasso is 38.1394, with a lambda chosen by CV of roughly 0.169 using glmnet. 

We can look a little bit more at some of the lars solutions with some other functions. The Cp statisics from the training data indicate we want the 5th solutions for both lasso and lars. There are functions that can make predictions and pull out the coefficients for those steps, or for particular lambdas, etc. The steps are most clearly seen in the summary of the object. You can do partial steps too. 

```{r}
lasso_pred <- predict(object1, newx = xBos[test_index,], s= 4, mode = "step")
predict(object1, type = "coefficients", s= 4, mode = "step")
mean((lasso_pred$fit - yBos.test)^2)
```

The test MSE is 39.8732 with the lasso fit from lars.

The lars fit is exactly the same in this case:
```{r}
lars_pred <- predict(object2, newx = xBos[test_index,], s= 4, mode = "step")
#predict(object2, type = "coefficients", s= 11, mode = "step")
mean((lars_pred$fit - yBos.test)^2)
```

These all look very comparable as solutions - the test MSEs don't differ very much. We used a step increment for the lasso solution from lars, which is different than choosing by cv, but you could check out cv.lars for its options. 

Based on these, the lowest test MSE is from the ridge fit by glmnet, so we'd propose that model. 


(c) Does your chosen model involve all of the features in the data set? Why or why not?

This will depend on what model you chose. Ridge cannot set coefs to 0, but Lasso can. For example, we can compare:

```{r}
predict(ridgeBos.mod, type = "coefficients", s = bestlamBos)[1:14,]
predict(lassoBos.mod, type = "coefficients", s = bestlamBosl)[1:14,]
```

Ridge won't set any coefs to 0, but if we had selected a lasso model it could set some coefficients to 0. 


